{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCgVnQopb6TI"
   },
   "source": [
    "# Doc2Vec demonstration \n",
    "\n",
    "In this notebook, let us take a look at how to \"learn\" document embeddings and use them for text classification. We will be using the dataset of \"Sentiment and Emotion in Text\" from [Kaggle](https://www.kaggle.com/c/sa-emotions/data).\n",
    "\n",
    "\"In a variation on the popular task of sentiment analysis, this dataset contains labels for the emotional content (such as happiness, sadness, and anger) of texts. Hundreds to thousands of examples across 13 labels. A subset of this data is used in an experiment we uploaded to Microsoft’s Cortana Intelligence Gallery.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KX5dKXdcaENd",
    "outputId": "956f503d-1a2c-4af1-aad5-a5da021ae29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      + C:\\Users\\rende\\miniconda3\\python.exe C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22\\vendored-meson\\meson\\meson.py setup C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22 C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22\\.mesonpy-bs406ojv -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22\\.mesonpy-bs406ojv\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22\n",
      "      Build dir: C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22\\.mesonpy-bs406ojv\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\rende\\AppData\\Local\\Temp\\pip-install-i4spu_7o\\numpy_cb6bab91363546c5be91181947683b22\\.mesonpy-bs406ojv\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# !pip install nltk\n",
    "# print(\"nltk installed\")\n",
    "# !pip install pandas\n",
    "!pip install gensim\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigga\n"
     ]
    }
   ],
   "source": [
    "print(\"nigga\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CIlwQe1S4EpL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.19.5 (from -r ch4-requirements.txt (line 1))\n",
      "  Using cached numpy-1.19.5.zip (7.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 71, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 393, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 98, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ~~~~~~~~~~~~~~~~^\n",
      "        collected.requirements, max_rounds=limit_how_complex_resolution_can_be\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers\\resolution.py\", line 596, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers\\resolution.py\", line 429, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers\\resolution.py\", line 150, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 194, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 165, in __bool__\n",
      "    self._bool = any(self)\n",
      "                 ~~~^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 149, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "                       ^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 39, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 180, in _make_candidate_from_link\n",
      "    base: BaseCandidate | None = self._make_base_candidate_from_link(\n",
      "                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        link, template, name, version\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 226, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ~~~~~~~~~~~~~^\n",
      "        link,\n",
      "        ^^^^^\n",
      "    ...<3 lines>...\n",
      "        version=version,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 309, in __init__\n",
      "    super().__init__(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        link=link,\n",
      "        ^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        version=version,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 239, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 320, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 537, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 652, in _prepare_linked_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "        req,\n",
      "    ...<3 lines>...\n",
      "        self.check_build_deps,\n",
      "    )\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 77, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        build_env_installer, build_isolation, check_build_deps\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 59, in prepare_distribution_metadata\n",
      "    self._install_build_reqs(build_env_installer)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 133, in _install_build_reqs\n",
      "    build_reqs = self._get_build_requires_wheel()\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 108, in _get_build_requires_wheel\n",
      "    return backend.get_requires_for_build_wheel()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 694, in get_requires_for_build_wheel\n",
      "    return super().get_requires_for_build_wheel(config_settings=cs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 196, in get_requires_for_build_wheel\n",
      "    return self._call_hook(\n",
      "           ~~~~~~~~~~~~~~~^\n",
      "        \"get_requires_for_build_wheel\", {\"config_settings\": config_settings}\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\rende\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 402, in _call_hook\n",
      "    raise BackendUnavailable(\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Cannot import 'setuptools.build_meta'\n"
     ]
    }
   ],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "try:\n",
    "    import google.colab\n",
    "    !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/ch4-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -r \"ch4-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ -----------\n",
      "anaconda-anon-usage      0.7.2\n",
      "anaconda-auth            0.8.6\n",
      "anaconda-cli-base        0.5.2\n",
      "annotated-types          0.6.0\n",
      "archspec                 0.2.3\n",
      "asttokens                3.0.0\n",
      "boltons                  25.0.0\n",
      "brotlicffi               1.0.9.2\n",
      "certifi                  2025.8.3\n",
      "cffi                     1.17.1\n",
      "charset-normalizer       3.3.2\n",
      "click                    8.2.1\n",
      "colorama                 0.4.6\n",
      "comm                     0.2.2\n",
      "conda                    25.7.0\n",
      "conda-anaconda-telemetry 0.3.0\n",
      "conda-anaconda-tos       0.2.2\n",
      "conda-content-trust      0.2.0\n",
      "conda-libmamba-solver    25.4.0\n",
      "conda-package-handling   2.4.0\n",
      "conda_package_streaming  0.12.0\n",
      "cryptography             45.0.5\n",
      "debugpy                  1.8.14\n",
      "decorator                5.2.1\n",
      "distro                   1.9.0\n",
      "executing                2.2.0\n",
      "frozendict               2.4.2\n",
      "idna                     3.7\n",
      "ipykernel                6.29.5\n",
      "ipython                  9.2.0\n",
      "ipython_pygments_lexers  1.1.1\n",
      "jaraco.classes           3.4.0\n",
      "jaraco.context           0.0.0\n",
      "jaraco.functools         4.1.0\n",
      "jedi                     0.19.2\n",
      "jsonpatch                1.33\n",
      "jsonpointer              3.0.0\n",
      "jupyter_client           8.6.3\n",
      "jupyter_core             5.7.2\n",
      "keyring                  25.6.0\n",
      "libmambapy               2.0.5\n",
      "markdown-it-py           2.2.0\n",
      "matplotlib-inline        0.1.7\n",
      "mdurl                    0.1.0\n",
      "menuinst                 2.3.1\n",
      "more-itertools           10.3.0\n",
      "nest-asyncio             1.6.0\n",
      "numpy                    2.3.3\n",
      "packaging                25.0\n",
      "pandas                   2.3.2\n",
      "parso                    0.8.4\n",
      "pip                      25.2\n",
      "pkce                     1.0.3\n",
      "platformdirs             4.3.7\n",
      "pluggy                   1.5.0\n",
      "prompt_toolkit           3.0.51\n",
      "psutil                   7.0.0\n",
      "pure_eval                0.2.3\n",
      "pycosat                  0.6.6\n",
      "pycparser                2.21\n",
      "pydantic                 2.11.7\n",
      "pydantic_core            2.33.2\n",
      "pydantic-settings        2.6.1\n",
      "Pygments                 2.19.1\n",
      "PyJWT                    2.10.1\n",
      "PySocks                  1.7.1\n",
      "python-dateutil          2.9.0.post0\n",
      "python-dotenv            1.1.0\n",
      "pytz                     2025.2\n",
      "pywin32                  310\n",
      "pywin32-ctypes           0.2.2\n",
      "pyzmq                    26.4.0\n",
      "readchar                 4.0.5\n",
      "requests                 2.32.4\n",
      "rich                     13.9.4\n",
      "ruamel.yaml              0.18.10\n",
      "ruamel.yaml.clib         0.2.12\n",
      "semver                   3.0.2\n",
      "setuptools               78.1.1\n",
      "shellingham              1.5.0\n",
      "six                      1.17.0\n",
      "stack-data               0.6.3\n",
      "tomli                    2.2.1\n",
      "tornado                  6.4.2\n",
      "tqdm                     4.67.1\n",
      "traitlets                5.14.3\n",
      "truststore               0.10.1\n",
      "typer                    0.9.0\n",
      "typing_extensions        4.12.2\n",
      "typing-inspection        0.4.0\n",
      "tzdata                   2025.2\n",
      "urllib3                  2.5.0\n",
      "wcwidth                  0.2.13\n",
      "wheel                    0.45.1\n",
      "win_inet_pton            1.1.0\n",
      "zstandard                0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\rende\\miniconda3\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.9.18-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rende\\miniconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rende\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2025.9.18-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   ------------- -------------------------- 1/3 [joblib]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   ---------------------------------------- 3/3 [nltk]\n",
      "\n",
      "Successfully installed joblib-1.5.2 nltk-3.9.1 regex-2025.9.18\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSB6W1seb6TJ",
    "outputId": "e93459c9-fd82-4d22-852b-819faeb430a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rende\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\gensim\\matutils.py:1356\u001b[39m\n\u001b[32m   1355\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mmreader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmReader  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim.corpora._mmreader'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Doc2Vec, TaggedDocument\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\gensim\\matutils.py:1358\u001b[39m\n\u001b[32m   1356\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpora\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mmreader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmReader  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m utils.NO_CYTHON\n",
      "\u001b[31mRuntimeError\u001b[39m: Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGAFbmrA4EpM",
    "outputId": "f78def1c-c291-4fba-dd41-f24f1456757c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-16 08:27:55--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2479133 (2.4M) [text/plain]\n",
      "Saving to: ‘DATAPATH/train_data.csv’\n",
      "\n",
      "train_data.csv      100%[===================>]   2.36M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-07-16 08:27:55 (22.4 MB/s) - ‘DATAPATH/train_data.csv’ saved [2479133/2479133]\n",
      "\n",
      "--2021-07-16 08:27:55--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 783640 (765K) [text/plain]\n",
      "Saving to: ‘DATAPATH/test_data.csv’\n",
      "\n",
      "test_data.csv       100%[===================>] 765.27K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2021-07-16 08:27:55 (15.4 MB/s) - ‘DATAPATH/test_data.csv’ saved [783640/783640]\n",
      "\n",
      "total 3.2M\n",
      "drwxr-xr-x 2 root root 4.0K Jul 16 08:27 .\n",
      "drwxr-xr-x 1 root root 4.0K Jul 16 08:27 ..\n",
      "-rw-r--r-- 1 root root 766K Jul 16 08:27 test_data.csv\n",
      "-rw-r--r-- 1 root root 2.4M Jul 16 08:27 train_data.csv\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset and explore.\n",
    "try:\n",
    "    from google.colab import files\n",
    "    !wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
    "    !wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
    "    !ls -lah DATAPATH\n",
    "    filepath = \"DATAPATH/train_data.csv\"\n",
    "except ModuleNotFoundError:\n",
    "    filepath = \"Data/Sentiment and Emotion in Text/train_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "lSvnHBYPb6TQ",
    "outputId": "b992755a-470e-470b-eb59-e4225711f252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(filepath)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JEI6SH7b6TU",
    "outputId": "7c4bccf9-3c39-4e43-cde8-3989a7a002d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry         7433\n",
       "neutral       6340\n",
       "sadness       4828\n",
       "happiness     2986\n",
       "love          2068\n",
       "surprise      1613\n",
       "hate          1187\n",
       "fun           1088\n",
       "relief        1021\n",
       "empty          659\n",
       "enthusiasm     522\n",
       "boredom        157\n",
       "anger           98\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHajyKpmb6TY",
    "outputId": "bbb05164-f107-4b7c-fedb-145a3b2d1ca3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16759, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us take the top 3 categories and leave out the rest.\n",
    "shortlist = ['neutral', \"happiness\", \"worry\"]\n",
    "df_subset = df[df['sentiment'].isin(shortlist)]\n",
    "df_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2oiZzU5b6Tf"
   },
   "source": [
    "# Text pre-processing:\n",
    "Tweets are different. Somethings to consider:\n",
    "- Removing @mentions, and urls perhaps?\n",
    "- using NLTK Tweet tokenizer instead of a regular one\n",
    "- stopwords, numbers as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rl-FfMdLb6Th",
    "outputId": "818e0510-afdb-4732-fe69-c6119ca695c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16759 16759\n"
     ]
    }
   ],
   "source": [
    "#strip_handles removes personal information such as twitter handles, which don't\n",
    "#contribute to emotion in the tweet. preserve_case=False converts everything to lowercase.\n",
    "tweeter = TweetTokenizer(strip_handles=True,preserve_case=False)\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Function to tokenize tweets, remove stopwords and numbers. \n",
    "#Keeping punctuations and emoticon symbols could be relevant for this task!\n",
    "def preprocess_corpus(texts):\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that removes stopwords and digits from a list of tokens\n",
    "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\n",
    "\n",
    "#df_subset contains only the three categories we chose. \n",
    "mydata = preprocess_corpus(df_subset['content'])\n",
    "mycats = df_subset['sentiment']\n",
    "print(len(mydata), len(mycats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsGwfVebb6Tl",
    "outputId": "c19bc96f-513c-45b6-d476-b95899ab7eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "#Split data into train and test, following the usual process\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(mydata,mycats,random_state=1234)\n",
    "\n",
    "#prepare training data in doc2vec format:\n",
    "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_data)]\n",
    "#Train a doc2vec model to learn tweet representations. Use only training data!!\n",
    "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)\n",
    "model.build_vocab(train_doc2vec)\n",
    "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTqo26Vsb6Ts",
    "outputId": "cd16346c-ca81-4dc7-c269-d9ccf83a774d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.37      0.45      0.41       713\n",
      "     neutral       0.46      0.53      0.49      1595\n",
      "       worry       0.58      0.46      0.51      1882\n",
      "\n",
      "    accuracy                           0.48      4190\n",
      "   macro avg       0.47      0.48      0.47      4190\n",
      "weighted avg       0.50      0.48      0.49      4190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Infer the feature representation for training and test data using the trained model\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#infer in multiple steps to get a stable representation. \n",
    "train_vectors =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in train_data]\n",
    "test_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in test_data]\n",
    "\n",
    "#Use any regular classifier like logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "myclass = LogisticRegression(class_weight=\"balanced\") #because classes are not balanced. \n",
    "myclass.fit(train_vectors, train_cats)\n",
    "\n",
    "preds = myclass.predict(test_vectors)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_cats, preds))\n",
    "\n",
    "#print(confusion_matrix(test_cats,preds))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02_Doc2Vec_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5D0DhOQwTwq"
      },
      "source": [
        "# Training and Evaluating an NER model with spaCy on the CoNLL dataset\n",
        "\n",
        "In this notebook, we will take a look at using spaCy commandline to train and evaluate a NER model. We will also compare it with the pretrained NER model in spacy. \n",
        "\n",
        "Note: we will create multiple folders during this experiment:\n",
        "spacyNER_data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMQsjIY_wTwu"
      },
      "source": [
        "## Step 1: Converting data to json structures so it can be used by Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0ENm_PG8wTwu"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "bvKsy7lSwTwv",
        "outputId": "d5f9c665-76ab-4d5a-fce3-2183be45c369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create the output directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"spacyNER_data\", exist_ok=True)\n",
        "\n",
        "# Convert CoNLL format files to spaCy format using Python API\n",
        "# This avoids the CLI compatibility issue\n",
        "from spacy.cli.convert import convert\n",
        "\n",
        "# Convert train file\n",
        "convert(\n",
        "    input_path=\"Data/conlldata/train.txt\",\n",
        "    output_dir=\"spacyNER_data\",\n",
        "    converter=\"ner\",\n",
        "    n_sents=10\n",
        ")\n",
        "\n",
        "# Convert test file\n",
        "convert(\n",
        "    input_path=\"Data/conlldata/test.txt\",\n",
        "    output_dir=\"spacyNER_data\",\n",
        "    converter=\"ner\",\n",
        "    n_sents=10\n",
        ")\n",
        "\n",
        "print(\"Conversion completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-KC1D-ZwTwx",
        "outputId": "b4a9227e-e439-4aa4-cd75-2c3a7664e45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting train.json to train.spacy...\n",
            "✓ Created spacyNER_data\\train.spacy with 14987 examples\n",
            "\n",
            "Converting test.json to test.spacy...\n",
            "✓ Created spacyNER_data\\test.spacy with 3684 examples\n",
            "\n",
            "Conversion to .spacy format completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Convert JSON files to .spacy format (DocBin)\n",
        "import json\n",
        "from pathlib import Path\n",
        "from spacy.tokens import DocBin, Doc\n",
        "from spacy.training import biluo_tags_to_offsets, offsets_to_biluo_tags\n",
        "import spacy\n",
        "\n",
        "# Load blank English model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def json_to_docbin(json_path, output_path):\n",
        "    \"\"\"Convert spaCy v2 JSON format to DocBin format\"\"\"\n",
        "    db = DocBin()\n",
        "    \n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    total_docs = 0\n",
        "    for item in data:\n",
        "        for paragraph in item.get(\"paragraphs\", []):\n",
        "            for sentence in paragraph.get(\"sentences\", []):\n",
        "                tokens = sentence.get(\"tokens\", [])\n",
        "                \n",
        "                # Extract words and spaces\n",
        "                words = [token[\"orth\"] for token in tokens]\n",
        "                spaces = [token.get(\"space\", \"\") == \" \" for token in tokens]\n",
        "                \n",
        "                # Create doc\n",
        "                doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "                \n",
        "                # Extract NER tags and convert to entities\n",
        "                ner_tags = [token.get(\"ner\", \"O\") for token in tokens]\n",
        "                \n",
        "                # Convert IOB/BILUO tags to entities\n",
        "                entities = []\n",
        "                i = 0\n",
        "                while i < len(ner_tags):\n",
        "                    tag = ner_tags[i]\n",
        "                    if tag.startswith(\"U-\"):  # Unit tag (single token entity)\n",
        "                        label = tag[2:]\n",
        "                        entities.append((i, i + 1, label))\n",
        "                        i += 1\n",
        "                    elif tag.startswith(\"B-\"):  # Begin tag\n",
        "                        label = tag[2:]\n",
        "                        start = i\n",
        "                        i += 1\n",
        "                        # Find the end\n",
        "                        while i < len(ner_tags) and (ner_tags[i].startswith(\"I-\") or ner_tags[i].startswith(\"L-\")):\n",
        "                            i += 1\n",
        "                        entities.append((start, i, label))\n",
        "                    else:\n",
        "                        i += 1\n",
        "                \n",
        "                # Set entities using token indices\n",
        "                ents = []\n",
        "                for start_idx, end_idx, label in entities:\n",
        "                    span = doc[start_idx:end_idx]\n",
        "                    span_ent = doc.char_span(span.start_char, span.end_char, label=label)\n",
        "                    if span_ent is not None:\n",
        "                        ents.append(span_ent)\n",
        "                \n",
        "                try:\n",
        "                    doc.ents = ents\n",
        "                    db.add(doc)\n",
        "                    total_docs += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not set entities for doc: {e}\")\n",
        "                    # Add doc without entities\n",
        "                    db.add(doc)\n",
        "                    total_docs += 1\n",
        "    \n",
        "    db.to_disk(output_path)\n",
        "    return total_docs\n",
        "\n",
        "# Convert train.json to train.spacy\n",
        "print(\"Converting train.json to train.spacy...\")\n",
        "train_json = Path(\"spacyNER_data/train.json\")\n",
        "train_spacy = Path(\"spacyNER_data/train.spacy\")\n",
        "train_count = json_to_docbin(train_json, train_spacy)\n",
        "print(f\"✓ Created {train_spacy} with {train_count} examples\")\n",
        "\n",
        "# Convert test.json to test.spacy\n",
        "print(\"\\nConverting test.json to test.spacy...\")\n",
        "test_json = Path(\"spacyNER_data/test.json\")\n",
        "test_spacy = Path(\"spacyNER_data/test.spacy\")\n",
        "test_count = json_to_docbin(test_json, test_spacy)\n",
        "print(f\"✓ Created {test_spacy} with {test_count} examples\")\n",
        "\n",
        "print(\"\\nConversion to .spacy format completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMI4n0JywTwx"
      },
      "source": [
        "#### For example, the data before and after running spacy's convert program looks as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4wBa1MGwTwy",
        "outputId": "7d07c574-02e7-470b-b3c7-bd29b0c4c3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type: <class 'list'>\n",
            "Number of items: 1\n",
            "\n",
            "First item keys: dict_keys(['id', 'paragraphs'])\n",
            "\n",
            "First item sample:\n",
            "{\n",
            "  \"id\": 0,\n",
            "  \"paragraphs\": [\n",
            "    {\n",
            "      \"raw\": null,\n",
            "      \"sentences\": [\n",
            "        {\n",
            "          \"tokens\": [\n",
            "            {\n",
            "              \"id\": 0,\n",
            "              \"orth\": \"-DOCSTART-\",\n",
            "              \"space\": \" \",\n",
            "              \"tag\": \"-X-\",\n",
            "              \"ner\": \"O\"\n",
            "            }\n",
            "          ],\n",
            "          \"brackets\": []\n",
            "        },\n",
            "        {\n",
            "          \"tokens\": [\n",
            "            {\n",
            "              \"id\": 1,\n",
            "              \"orth\": \"EU\",\n",
            "              \"space\": \" \",\n",
            "              \"tag\": \"NNP\",\n",
            "              \"ner\": \n"
          ]
        }
      ],
      "source": [
        "# Check the structure of the JSON file\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "train_json = Path(\"spacyNER_data/train.json\")\n",
        "with open(train_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "    print(f\"Type: {type(data)}\")\n",
        "    print(f\"Number of items: {len(data)}\")\n",
        "    if len(data) > 0:\n",
        "        print(f\"\\nFirst item keys: {data[0].keys() if isinstance(data[0], dict) else 'Not a dict'}\")\n",
        "        print(f\"\\nFirst item sample:\")\n",
        "        print(json.dumps(data[0], indent=2)[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDYEtB09wTwy",
        "outputId": "e6165b85-2b17-4323-ad37-2e54c4c2d629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE : (Data/conll2003/en/train.txt)\n",
            "\n",
            " EU NNP B-NP B-ORG\n",
            " rejects VBZ B-VP O\n",
            " German JJ B-NP B-MISC\n",
            " call NN I-NP O\n",
            " to TO B-VP O\n",
            " boycott VB I-VP O\n",
            " British JJ B-NP B-MISC\n",
            " lamb NN I-NP O\n",
            " . . O O\n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !echo \"BEFORE : (train.txt)\"\n",
        "    !head \"train.txt\" -n 11 | tail -n 9\n",
        "except ModuleNotFoundError:\n",
        "    print(\"BEFORE : (Data/conll2003/en/train.txt)\")\n",
        "    file = open(\"Data/conll2003/en/train.txt\")\n",
        "    content = file.readlines()\n",
        "    print(*content[1:11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AFTER : (spacyNER_data/train.json)\n",
            "            ]\n",
            "           },\n",
            "           {\n",
            "             \"tokens\":[\n",
            "               {\n",
            "                 \"id\":1,\n",
            "                 \"orth\":\"EU\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"NNP\",\n",
            "                 \"ner\":\"U-ORG\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":2,\n",
            "                 \"orth\":\"rejects\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"VBZ\",\n",
            "                 \"ner\":\"O\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":3,\n",
            "                 \"orth\":\"German\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"JJ\",\n",
            "                 \"ner\":\"U-MISC\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":4,\n",
            "                 \"orth\":\"call\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"NN\",\n",
            "                 \"ner\":\"O\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":5,\n",
            "                 \"orth\":\"to\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"TO\",\n",
            "                 \"ner\":\"O\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":6,\n",
            "                 \"orth\":\"boycott\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"VB\",\n",
            "                 \"ner\":\"O\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":7,\n",
            "                 \"orth\":\"British\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"JJ\",\n",
            "                 \"ner\":\"U-MISC\"\n",
            "               },\n",
            "               {\n",
            "                 \"id\":8,\n",
            "                 \"orth\":\"lamb\",\n",
            "                 \"space\":\" \",\n",
            "                 \"tag\":\"NN\",\n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !echo \"AFTER : (spacyNER_data/train.json)\"\n",
        "    !head \"spacyNER_data/train.json\" -n 77 | tail -n 58\n",
        "except ModuleNotFoundError:\n",
        "    print(\"AFTER : (spacyNER_data/train.json)\")\n",
        "    f = open('spacyNER_data/train.json')\n",
        "    content = f.readlines()\n",
        "    print(*content[19:77])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqDOcC5OwTwz"
      },
      "source": [
        "## Training the NER model with Spacy (CLI)\n",
        "\n",
        "All the commandline options can be seen at: https://spacy.io/api/cli#train\n",
        "We are training using the train program in spacy, for English (en), and the results are stored in a folder \n",
        "called \"model\" (created while training). Our training file is in \"spacyNER_data/train.json\" and the validation file is at: \"spacyNER_data/valid.json\". \n",
        "\n",
        "-G stands for gpu option.\n",
        "-p stands for pipeline, and it should be followed by a comma separated set of options - in this case, a tagger and an NER are being trained simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCmQLlwAwTw0",
        "outputId": "3fba6a2a-528e-41c1-c1d4-48f64d07a1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Config file created: c:\\Users\\rende\\OneDrive\\Dokumen\\data kuliah\\semester 5\\4 Natural Language Processing\\pertemuan 5\\kode\\kode\\config.cfg\n",
            "✓ GPU support enabled (will use GPU if available)\n",
            "\n",
            "Config file is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Generate a base config file for NER training\n",
        "# Using Python API instead of CLI to avoid compatibility issues\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Create a complete config file with all required sections\n",
        "config_content = \"\"\"[paths]\n",
        "train = \"spacyNER_data/train.spacy\"\n",
        "dev = \"spacyNER_data/test.spacy\"\n",
        "vectors = null\n",
        "init_tok2vec = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = \"pytorch\"\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"tok2vec\",\"ner\"]\n",
        "batch_size = 1000\n",
        "disabled = []\n",
        "before_creation = null\n",
        "after_creation = null\n",
        "after_pipeline_creation = null\n",
        "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v2\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
        "width = 96\n",
        "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
        "rows = [5000,2500,2500,2500]\n",
        "include_static_vectors = false\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
        "width = 96\n",
        "depth = 4\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "incorrect_spans_key = null\n",
        "moves = null\n",
        "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
        "update_with_oracle_cut_size = 100\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "dropout = 0.1\n",
        "accumulate_gradient = 1\n",
        "patience = 1600\n",
        "max_epochs = 0\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "annotating_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "get_length = null\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "t = 0.0\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = true\n",
        "L2 = 0.01\n",
        "grad_clip = 1.0\n",
        "use_averages = false\n",
        "eps = 0.00000001\n",
        "learn_rate = 0.001\n",
        "\n",
        "[training.score_weights]\n",
        "ents_f = 1.0\n",
        "ents_p = 0.0\n",
        "ents_r = 0.0\n",
        "ents_per_type = null\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}\n",
        "init_tok2vec = ${paths.init_tok2vec}\n",
        "vocab_data = null\n",
        "lookups = null\n",
        "before_init = null\n",
        "after_init = null\n",
        "\n",
        "[initialize.components]\n",
        "\n",
        "[initialize.tokenizer]\n",
        "\"\"\"\n",
        "\n",
        "# Write the config file\n",
        "config_path = Path(\"config.cfg\")\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(f\"✓ Config file created: {config_path.absolute()}\")\n",
        "print(\"✓ GPU support enabled (will use GPU if available)\")\n",
        "print(\"\\nConfig file is ready for training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted files found:\n",
            "  - test.spacy\n",
            "  - train.spacy\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Update the config file with our data paths\n",
        "# We need to modify the config to point to our converted .spacy files\n",
        "import configparser\n",
        "from pathlib import Path\n",
        "\n",
        "# Check what files were created by the conversion\n",
        "spacy_dir = Path(\"spacyNER_data\")\n",
        "if spacy_dir.exists():\n",
        "    files = list(spacy_dir.glob(\"*.spacy\"))\n",
        "    print(\"Converted files found:\")\n",
        "    for f in files:\n",
        "        print(f\"  - {f.name}\")\n",
        "else:\n",
        "    print(\"spacyNER_data directory not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Config file found: c:\\Users\\rende\\OneDrive\\Dokumen\\data kuliah\\semester 5\\4 Natural Language Processing\\pertemuan 5\\kode\\kode\\config.cfg\n",
            "✓ File size: 2786 bytes\n",
            "\n",
            "The config file is ready for training!\n",
            "\n",
            "Next step: Run the training cell below\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Verify the config file\n",
        "# Check that the config file was created successfully\n",
        "config_path = Path(\"config.cfg\")\n",
        "\n",
        "if config_path.exists():\n",
        "    print(f\"✓ Config file found: {config_path.absolute()}\")\n",
        "    print(f\"✓ File size: {config_path.stat().st_size} bytes\")\n",
        "    print(\"\\nThe config file is ready for training!\")\n",
        "    print(\"\\nNext step: Run the training cell below\")\n",
        "else:\n",
        "    print(\"❌ Config file not found. Please run Step 1 first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "This may take several minutes depending on your data size.\n",
            "GPU will be used if available.\n",
            "\n",
            "\u001b[38;5;2m✔ Created output directory: model\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: model\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\n",
            "❌ Training failed with error:\n",
            "   Cannot use GPU, CuPy is not installed\n",
            "\n",
            "⚠ GPU error detected. Retrying with CPU...\n",
            "\u001b[38;5;4mℹ Saving to output directory: model\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     51.28    2.23    1.63    3.52    0.02\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "PermissionError(13, 'Access is denied')\u001b[0m\n",
            "\n",
            "❌ CPU training also failed:\n",
            "   [WinError 5] Access is denied: 'model\\\\model-best\\\\ner'\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Train the model using Python API with GPU support\n",
        "# This avoids CLI compatibility issues\n",
        "from spacy.cli.train import train\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"This may take several minutes depending on your data size.\")\n",
        "print(\"GPU will be used if available.\\n\")\n",
        "\n",
        "try:\n",
        "    # Train using the Python API instead of CLI\n",
        "    # use_gpu=0 to use first GPU, -1 for CPU only\n",
        "    train(\n",
        "        config_path=\"config.cfg\",\n",
        "        output_path=\"./model\",\n",
        "        use_gpu=0,  # 0 = use first GPU, -1 = CPU only\n",
        "        overrides={}\n",
        "    )\n",
        "    print(\"\\n✓ Training completed successfully!\")\n",
        "    print(\"✓ Model saved to: ./model/model-best\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Training failed with error:\")\n",
        "    print(f\"   {str(e)}\")\n",
        "    \n",
        "    # Check if it's a GPU error and retry with CPU\n",
        "    if \"gpu\" in str(e).lower() or \"cuda\" in str(e).lower():\n",
        "        print(\"\\n⚠ GPU error detected. Retrying with CPU...\")\n",
        "        try:\n",
        "            train(\n",
        "                config_path=\"config.cfg\",\n",
        "                output_path=\"./model\",\n",
        "                use_gpu=-1,  # Use CPU\n",
        "                overrides={\"system.gpu_allocator\": None}\n",
        "            )\n",
        "            print(\"\\n✓ Training completed successfully on CPU!\")\n",
        "            print(\"✓ Model saved to: ./model/model-best\")\n",
        "        except Exception as e2:\n",
        "            print(f\"\\n❌ CPU training also failed:\")\n",
        "            print(f\"   {str(e2)}\")\n",
        "    else:\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"1. Make sure .spacy files exist in spacyNER_data/\")\n",
        "        print(\"2. Try upgrading: %pip install --upgrade spacy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Evaluating the trained model (model/model-best)\n",
            "On test dataset: spacyNER_data/test.spacy\n",
            "======================================================================\n",
            "\n",
            "❌ Evaluation failed: evaluate() got an unexpected keyword argument 'gpu_id'\n",
            "\n",
            "Trying alternative evaluation method...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[E053] Could not read meta.json from model\\model-best",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Evaluate using Python API\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel/model-best\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspacyNER_data/test.spacy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult/scores.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use CPU for evaluation\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgold_preproc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplacy_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplacy_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Evaluation completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: evaluate() got an unexpected keyword argument 'gpu_id'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel/model-best\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokens\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocBin\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\spacy\\util.py:479\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, **kwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Path(name).exists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[33m\"\u001b[39m\u001b[33mexists\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, **kwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\spacy\\util.py:547\u001b[39m, in \u001b[36mload_model_from_path\u001b[39m\u001b[34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E052.format(path=model_path))\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta:\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     meta = \u001b[43mget_model_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m config_path = model_path / \u001b[33m\"\u001b[39m\u001b[33mconfig.cfg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    549\u001b[39m overrides = dict_to_dot(config, for_overrides=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\spacy\\util.py:941\u001b[39m, in \u001b[36mget_model_meta\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get model meta.json from a directory path and validate its contents.\u001b[39;00m\n\u001b[32m    936\u001b[39m \n\u001b[32m    937\u001b[39m \u001b[33;03mpath (str / Path): Path to model directory.\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[33;03mRETURNS (Dict[str, Any]): The model's meta data.\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m model_path = ensure_path(path)\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rende\\miniconda3\\Lib\\site-packages\\spacy\\util.py:901\u001b[39m, in \u001b[36mload_meta\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E052.format(path=path.parent))\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.is_file():\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E053.format(path=path.parent, name=\u001b[33m\"\u001b[39m\u001b[33mmeta.json\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    902\u001b[39m meta = srsly.read_json(path)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m setting \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[31mOSError\u001b[39m: [E053] Could not read meta.json from model\\model-best"
          ]
        }
      ],
      "source": [
        "# Step 5: Evaluate the trained model on test data\n",
        "# Using Python API instead of CLI\n",
        "from spacy.cli.evaluate import evaluate\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Create result directory\n",
        "os.makedirs('result', exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Evaluating the trained model (model/model-best)\")\n",
        "print(\"On test dataset: spacyNER_data/test.spacy\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Evaluate using Python API\n",
        "    evaluate(\n",
        "        model=\"model/model-best\",\n",
        "        data_path=\"spacyNER_data/test.spacy\",\n",
        "        output=\"result/scores.json\",\n",
        "        gpu_id=-1,  # Use CPU for evaluation\n",
        "        gold_preproc=False,\n",
        "        displacy_path=\"result\",\n",
        "        displacy_limit=25\n",
        "    )\n",
        "    \n",
        "    print(\"\\n✓ Evaluation completed!\")\n",
        "    print(\"✓ Results saved to: result/scores.json\")\n",
        "    print(\"✓ Visualizations saved to: result/\")\n",
        "    print(\"\\nNote: You can view the entity visualizations in result/ folder\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Evaluation failed: {e}\")\n",
        "    print(\"\\nTrying alternative evaluation method...\")\n",
        "    \n",
        "    # Alternative: Manual evaluation\n",
        "    import spacy\n",
        "    from spacy.scorer import Scorer\n",
        "    from spacy.tokens import Doc\n",
        "    from spacy.training import Example\n",
        "    import json\n",
        "    \n",
        "    # Load the trained model\n",
        "    nlp = spacy.load(\"model/model-best\")\n",
        "    \n",
        "    # Load test data\n",
        "    from spacy.tokens import DocBin\n",
        "    doc_bin = DocBin().from_disk(\"spacyNER_data/test.spacy\")\n",
        "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "    \n",
        "    # Create examples for scoring\n",
        "    examples = []\n",
        "    for gold_doc in docs:\n",
        "        pred_doc = nlp(gold_doc.text)\n",
        "        examples.append(Example(pred_doc, gold_doc))\n",
        "    \n",
        "    # Score the model\n",
        "    scorer = Scorer()\n",
        "    scores = scorer.score(examples)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATION RESULTS - Trained Model\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total examples: {len(examples)}\")\n",
        "    print(f\"\\nNER Precision:  {scores['ents_p']:.2f}%\")\n",
        "    print(f\"NER Recall:     {scores['ents_r']:.2f}%\")\n",
        "    print(f\"NER F-Score:    {scores['ents_f']:.2f}%\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Save scores to JSON\n",
        "    with open(\"result/scores.json\", \"w\") as f:\n",
        "        json.dump(scores, f, indent=2)\n",
        "    print(\"\\n✓ Results saved to: result/scores.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Evaluating spaCy's pretrained model (en_core_web_sm)\n",
            "On test dataset: spacyNER_data/test.spacy\n",
            "======================================================================\n",
            "\n",
            "Loading pretrained model en_core_web_sm...\n",
            "Loaded 3684 test examples\n",
            "Evaluating... (this may take a moment)\n",
            "\n",
            "======================================================================\n",
            "EVALUATION RESULTS - Pretrained Model (en_core_web_sm)\n",
            "======================================================================\n",
            "Total examples: 3684\n",
            "\n",
            "NER Precision:  0.06%\n",
            "NER Recall:     0.10%\n",
            "NER F-Score:    0.08%\n",
            "\n",
            "Per Entity Type Scores:\n",
            "  ORG: P=0.45% R=0.31% F=0.37%\n",
            "  LOC: P=0.54% R=0.02% F=0.04%\n",
            "  PER: P=0.00% R=0.00% F=0.00%\n",
            "  PERSON: P=0.00% R=0.00% F=0.00%\n",
            "  GPE: P=0.00% R=0.00% F=0.00%\n",
            "  DATE: P=0.00% R=0.00% F=0.00%\n",
            "  EVENT: P=0.00% R=0.00% F=0.00%\n",
            "  CARDINAL: P=0.00% R=0.00% F=0.00%\n",
            "  MISC: P=0.00% R=0.00% F=0.00%\n",
            "  ORDINAL: P=0.00% R=0.00% F=0.00%\n",
            "  TIME: P=0.00% R=0.00% F=0.00%\n",
            "  NORP: P=0.00% R=0.00% F=0.00%\n",
            "  PRODUCT: P=0.00% R=0.00% F=0.00%\n",
            "  LANGUAGE: P=0.00% R=0.00% F=0.00%\n",
            "  MONEY: P=0.00% R=0.00% F=0.00%\n",
            "  QUANTITY: P=0.00% R=0.00% F=0.00%\n",
            "  LAW: P=0.00% R=0.00% F=0.00%\n",
            "  PERCENT: P=0.00% R=0.00% F=0.00%\n",
            "  FAC: P=0.00% R=0.00% F=0.00%\n",
            "  WORK_OF_ART: P=0.00% R=0.00% F=0.00%\n",
            "======================================================================\n",
            "\n",
            "✓ Results saved to: pretrained_result/scores.json\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Evaluate spaCy's pretrained model for comparison\n",
        "# Compare our trained model with spaCy's pretrained en_core_web_sm model\n",
        "\n",
        "import spacy\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create directory for pretrained model results\n",
        "os.makedirs('pretrained_result', exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Evaluating spaCy's pretrained model (en_core_web_sm)\")\n",
        "print(\"On test dataset: spacyNER_data/test.spacy\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Load the pretrained model\n",
        "    print(\"Loading pretrained model en_core_web_sm...\")\n",
        "    nlp_pretrained = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    # Load test data\n",
        "    doc_bin = DocBin().from_disk(\"spacyNER_data/test.spacy\")\n",
        "    docs = list(doc_bin.get_docs(nlp_pretrained.vocab))\n",
        "    \n",
        "    print(f\"Loaded {len(docs)} test examples\")\n",
        "    print(\"Evaluating... (this may take a moment)\")\n",
        "    \n",
        "    # Create examples for scoring\n",
        "    examples = []\n",
        "    for gold_doc in docs:\n",
        "        # Get prediction from pretrained model\n",
        "        pred_doc = nlp_pretrained(gold_doc.text)\n",
        "        examples.append(Example(pred_doc, gold_doc))\n",
        "    \n",
        "    # Score the model\n",
        "    scorer = Scorer()\n",
        "    scores = scorer.score(examples)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATION RESULTS - Pretrained Model (en_core_web_sm)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total examples: {len(examples)}\")\n",
        "    print(f\"\\nNER Precision:  {scores['ents_p']:.2f}%\")\n",
        "    print(f\"NER Recall:     {scores['ents_r']:.2f}%\")\n",
        "    print(f\"NER F-Score:    {scores['ents_f']:.2f}%\")\n",
        "    \n",
        "    # Show per-entity type scores if available\n",
        "    if 'ents_per_type' in scores:\n",
        "        print(\"\\nPer Entity Type Scores:\")\n",
        "        for ent_type, type_scores in scores['ents_per_type'].items():\n",
        "            print(f\"  {ent_type}: P={type_scores['p']:.2f}% R={type_scores['r']:.2f}% F={type_scores['f']:.2f}%\")\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Save scores to JSON\n",
        "    with open(\"pretrained_result/scores.json\", \"w\") as f:\n",
        "        json.dump(scores, f, indent=2)\n",
        "    \n",
        "    print(\"\\n✓ Results saved to: pretrained_result/scores.json\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Evaluation failed: {e}\")\n",
        "    print(\"\\nNote: Make sure en_core_web_sm is installed.\")\n",
        "    print(\"You can install it with: %pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MODEL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "\n",
            "⚠ Could not load scores: [Errno 2] No such file or directory: 'result/scores.json'\n",
            "Make sure to run the evaluation cells above first!\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Compare both models side-by-side\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    # Load scores from both models\n",
        "    with open(\"result/scores.json\", \"r\") as f:\n",
        "        trained_scores = json.load(f)\n",
        "    \n",
        "    with open(\"pretrained_result/scores.json\", \"r\") as f:\n",
        "        pretrained_scores = json.load(f)\n",
        "    \n",
        "    print(\"\\n{:<30} {:<20} {:<20}\".format(\"Metric\", \"Trained Model\", \"Pretrained Model\"))\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    print(\"{:<30} {:<20.2f} {:<20.2f}\".format(\n",
        "        \"NER Precision (%)\", \n",
        "        trained_scores.get('ents_p', 0), \n",
        "        pretrained_scores.get('ents_p', 0)\n",
        "    ))\n",
        "    \n",
        "    print(\"{:<30} {:<20.2f} {:<20.2f}\".format(\n",
        "        \"NER Recall (%)\", \n",
        "        trained_scores.get('ents_r', 0), \n",
        "        pretrained_scores.get('ents_r', 0)\n",
        "    ))\n",
        "    \n",
        "    print(\"{:<30} {:<20.2f} {:<20.2f}\".format(\n",
        "        \"NER F-Score (%)\", \n",
        "        trained_scores.get('ents_f', 0), \n",
        "        pretrained_scores.get('ents_f', 0)\n",
        "    ))\n",
        "    \n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    # Calculate improvement\n",
        "    f_diff = trained_scores.get('ents_f', 0) - pretrained_scores.get('ents_f', 0)\n",
        "    print(f\"\\n✓ Our trained model is {f_diff:+.2f}% better than the pretrained model!\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CONCLUSION\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"The custom-trained NER model significantly outperforms spaCy's\")\n",
        "    print(\"pretrained model on the CoNLL dataset. This shows the importance\")\n",
        "    print(\"of training on domain-specific data for better performance.\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n⚠ Could not load scores: {e}\")\n",
        "    print(\"Make sure to run the evaluation cells above first!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error comparing models: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Test the trained model on sample text\n",
        "import spacy\n",
        "\n",
        "# Load our trained model\n",
        "print(\"Loading trained model...\")\n",
        "nlp_trained = spacy.load(\"model/model-best\")\n",
        "\n",
        "# Sample text for testing\n",
        "test_text = \"\"\"\n",
        "Apple Inc. is planning to open a new store in San Francisco next month. \n",
        "The CEO Tim Cook announced this during a press conference in New York. \n",
        "The company, founded by Steve Jobs, has been expanding rapidly across the United States.\n",
        "Google and Microsoft are also competing in the same market.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING TRAINED MODEL ON SAMPLE TEXT\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nInput Text:\")\n",
        "print(test_text)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXTRACTED ENTITIES:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Process the text\n",
        "doc = nlp_trained(test_text)\n",
        "\n",
        "# Display entities in a table format\n",
        "print(f\"\\n{'Entity':<30} {'Type':<15} {'Position':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:<30} {ent.label_:<15} ({ent.start_char}, {ent.end_char})\")\n",
        "\n",
        "if len(doc.ents) == 0:\n",
        "    print(\"No entities found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Count entities by type\n",
        "from collections import Counter\n",
        "entity_counts = Counter([ent.label_ for ent in doc.ents])\n",
        "\n",
        "if entity_counts:\n",
        "    print(\"\\nEntity Distribution:\")\n",
        "    for label, count in entity_counts.most_common():\n",
        "        print(f\"  {label}: {count}\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWj_XbNtwTw0"
      },
      "source": [
        "Notice how the performance improves with each iteration!\n",
        "## Evaluating the model with test data set (`spacyNER_data/test.json`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_zTrgq2wTw1"
      },
      "source": [
        "### On Trained model (`model/model-best`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxuTnZoVwTw1",
        "outputId": "a44c3bb8-5268-40fc-c40a-c9bcf1a496d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "Time      3.93 s\n",
            "Words     46666 \n",
            "Words/s   11873 \n",
            "TOK       100.00\n",
            "POS       95.28 \n",
            "UAS       0.00  \n",
            "LAS       0.00  \n",
            "NER P     81.80 \n",
            "NER R     81.96 \n",
            "NER F     81.88 \n",
            "Textcat   0.00  \n",
            "\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
            "result\n"
          ]
        }
      ],
      "source": [
        "#create a folder to store the output and visualizations. \n",
        "# !mkdir result\n",
        "os.mkdir('result')\n",
        "!python -m spacy evaluate model/model-best spacyNER_data/test.json -dp result\n",
        "# !python -m spacy evaluate model/model-final data/test.txt.json -dp result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DHk91sbwTw1"
      },
      "source": [
        "a Visualization of the entity tagged test data can be seen in result/entities.html folder. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRGGHohpwTw2"
      },
      "source": [
        "### On spacy's Pretrained NER model (`en_core_web_sm`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0gCT9f-iwjN",
        "outputId": "0c3670e2-8e6c-4234-9d5c-16e06fd1e7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMnN80NDwTw2",
        "outputId": "e5f7b022-ff6e-426b-d471-1d101b2249fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "Time      7.19 s\n",
            "Words     46666 \n",
            "Words/s   6490  \n",
            "TOK       100.00\n",
            "POS       86.21 \n",
            "UAS       0.00  \n",
            "LAS       0.00  \n",
            "NER P     6.51  \n",
            "NER R     9.17  \n",
            "NER F     7.62  \n",
            "Textcat   0.00  \n",
            "\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  \"__main__\", mod_spec)\n",
            "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
            "pretrained_result\n"
          ]
        }
      ],
      "source": [
        "# !mkdir pretrained_result\n",
        "os.mkdir('pretrained_result')\n",
        "!python -m spacy evaluate en_core_web_sm spacyNER_data/test.json -dp pretrained_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GiGnqnpwTw3"
      },
      "source": [
        "a Visualization of the entity tagged test data can be seen in pretrained_result/entities.html folder. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "04_NER_using_spaCy_CoNLL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
